{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ef94b4-7041-4f1f-9993-6a2045426d64",
   "metadata": {},
   "source": [
    "# La Bibliothèque NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79003c61-07c2-429e-8c34-af6c6093ba56",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307fa203-da62-4f99-8cdf-03554396b882",
   "metadata": {},
   "source": [
    "* Le corpus est un ensemble de documents dans le même sujet.\n",
    "* Un document est composé de paragraphes, de phrases et des mots.\n",
    "* Le paragraphe est composé de phrases.\n",
    "* La phrase est composée d'un ensemble de mots, de symboles (ponctuations, ...).\n",
    "* Le mot est une chaine de caractères.\n",
    "* Un chaine de caractère est composée de caractères."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a8fd0-9d8b-4870-b8c4-96ebbb48a5a7",
   "metadata": {},
   "source": [
    "## Installation de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18bedd-1115-4c53-aa87-23db23da53a2",
   "metadata": {},
   "source": [
    "* Outil Conda : \n",
    "<br>*conda install nltk*\n",
    "* Outil pip :\n",
    "<br>*pip install nltk*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910c6f9-7b2e-4e69-9ca2-3f2a2926d1f6",
   "metadata": {},
   "source": [
    "## Importation de NLTK\n",
    "### import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85043902-d69e-4118-ad9d-7b982d7d7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impoeter la bibliothèque NLTK / Framework spacy  / stanfordnlp\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a454c-6593-4419-ad06-f386ea272b0d",
   "metadata": {},
   "source": [
    "### Télécharger les modules nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078abbdf-f629-4b84-a416-f3baeab60e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3d47e-eee3-4063-aaef-0efc1363ac5c",
   "metadata": {},
   "source": [
    "### Télécharger les packages nltk nécessaire \n",
    "punkt : Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d046aaaf-f0b5-41c4-88a2-07ce1569fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student-\n",
      "[nltk_data]     cum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/student-cum/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Télécharger les packages nltk nécessaire \n",
    "nltk.download('punkt')  # punkt pour la tokenisation\n",
    "nltk.download('averaged_perceptron_tagger')   #pour les POS tagging (Part of speech tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a45110-5355-4709-b0bf-cb219a4f0910",
   "metadata": {},
   "source": [
    "## Etape 1 : La Tokenization du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3dbb2e-e388-4d8c-965b-cbc6ac80bb0e",
   "metadata": {},
   "source": [
    "### Importer les modules de tokenisation\n",
    "* word_tokenize : donne une listes de jetons, sous forme de mots, d'une chaines de caractères\n",
    "* sent_tokenize : donne une liste de phrases du document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40a47eb-503c-4beb-bccc-dfd3bc0f2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340fc667-013b-49d7-ab5f-4b298afb38dd",
   "metadata": {},
   "source": [
    "## Initialisation de la variable texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63245460-0126-49c4-a572-fa7ee8ccd00d",
   "metadata": {},
   "source": [
    "### Texte sur plusieurs lignes : utilisez les trois (quotes, ou double quots) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aedb9467-ab5a-4ee7-82ae-97f2d0eb733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"One of the most significant contributions of technology is its ability to enhance productivity. Automation and digital \n",
    "tools enable businesses to streamline processes, reduce costs, and increase output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b564f68-e788-4d63-9c52-f6e29f5029e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texte = \"One of the most significant contributions of technology is its ability to enhance productivity. Automation and digital tools enable businesses to streamline processes, reduce costs, and increase output.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5feda1-a70a-4b3b-9fde-d09dda20a5e4",
   "metadata": {},
   "source": [
    "### Appel des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c98d75-6032-4de1-bd60-c63276bf7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenz = nltk.word_tokenize(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7425446-c557-441c-a901-1c4489adb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(texte)  # Tokenisation\n",
    "phrases = sent_tokenize(texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba00a94-33f4-4caa-ae8d-11c108c16b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens :\n",
      "['One', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', '.', 'Automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', ',', 'reduce', 'costs', ',', 'and', 'increase', 'output', '.']\n",
      "\n",
      "Le nombres de Tokens = 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens : mots\n",
    "print(f\"La liste des Tokens :\\n{tokens}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens\n",
    "print(f\"Le nombres de Tokens = {len(tokens)}\\n\") #len(ch) length = longeur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27138b57-830b-4c47-a6ed-0cf7f4b4552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Phrases :\n",
      "['One of the most significant contributions of technology is its ability to enhance productivity.', 'Automation and digital tools enable businesses to streamline processes, reduce costs, and increase output.']\n",
      "\n",
      "Le nombres de Phrases = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Afficher les Phrases\n",
    "print(f\"La liste des Phrases :\\n{phrases}\\n\")\n",
    "\n",
    "#Afficher le nombre de Phrases\n",
    "print(f\"Le nombres de Phrases = {len(phrases)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612acbd3-68af-4565-8661-309abab3520c",
   "metadata": {},
   "source": [
    "## Etape 2 : Supprimer la ponctuation\n",
    "### Importer le module PunktToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aefcc069-2dae-4bed-8ffc-76d01bb3ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktToken\n",
    "# PunktToken --> fct : is_non_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512ea0d-e326-44bf-ba58-cdea54beac4b",
   "metadata": {},
   "source": [
    "#### Parcours par element\n",
    "#### parcours par indice (index)\n",
    "\n",
    "Liste [elt1, elt2, elt3, ...]\n",
    "<br>index   0      1    2    3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fa557-910b-4f62-988d-b19bfdbb38cf",
   "metadata": {},
   "source": [
    "#### Utiliser deux méthodes \n",
    "1. Une boule FOR ordinnaire\n",
    "2. List Comprehension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "695eac0c-8714-4425-8be2-1d4a62576f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des tokens :\n",
      "['One', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', '.', 'Automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', ',', 'reduce', 'costs', ',', 'and', 'increase', 'output', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"La liste des tokens :\\n{tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f94403-247a-4a1a-8457-375782cf4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle for\n",
    "# La liste des token : tokens\n",
    "token_ponct = []\n",
    "\n",
    "for token in tokens :\n",
    "    if PunktToken(token).is_non_punct :\n",
    "        token_ponct.append(token)  # Ajouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ccf4bd-7db7-4f24-84fd-bd5818641b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens sans ponct :\n",
      "['One', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', 'Automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', 'reduce', 'costs', 'and', 'increase', 'output']\n",
      "\n",
      "Le nombres de tokens sans ponct = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens : mots\n",
    "print(f\"La liste des Tokens sans ponct :\\n{token_ponct}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens\n",
    "print(f\"Le nombres de tokens sans ponct = {len(token_ponct)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b07f4-7159-4e7b-8dbe-2e26ba3ded97",
   "metadata": {},
   "source": [
    "#### List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65bc3708-911f-455d-a4ab-63378d203a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Comprehension\n",
    "\n",
    "tokens_ponct = [jeton for jeton in tokens if PunktToken(jeton).is_non_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08da1de0-f48f-420e-8522-29f3d0f3f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens sans ponctuations :\n",
      "['One', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', 'Automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', 'reduce', 'costs', 'and', 'increase', 'output']\n",
      "\n",
      "Le nombres de tokens sans ponctuations = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens : mots\n",
    "print(f\"La liste des Tokens sans ponctuations :\\n{tokens_ponct}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens\n",
    "print(f\"Le nombres de tokens sans ponctuations = {len(tokens_ponct)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04cd2b3-d11d-4ad5-99d2-cdf388f9d19c",
   "metadata": {},
   "source": [
    "## Etape 3 : Convertir en minuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f06571c-31f7-4a0c-8bd0-2434f993de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle for\n",
    "token_min = []\n",
    "for jeton in tokens_ponct :\n",
    "    token_min.append(jeton.lower())  #Ajouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d10e0c-b42b-40a9-8529-77ab227b6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Comprehension\n",
    "tokens_min = [token.lower() for token in tokens_ponct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbffe04d-b659-481c-8623-e7c5fca195f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens en minuscule :\n",
      "['one', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', 'automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', 'reduce', 'costs', 'and', 'increase', 'output']\n",
      "\n",
      "Le nombres de tokens en minuscule = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens : mots\n",
    "print(f\"La liste des Tokens en minuscule :\\n{tokens_min}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens\n",
    "print(f\"Le nombres de tokens en minuscule = {len(tokens_min)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2bc73-c1da-43a4-ae5c-998b40105ae5",
   "metadata": {},
   "source": [
    "## Etape 4 : Suppression des mots vides\n",
    "### Importer le module stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "facd8c41-d34b-4911-a3e2-9980773976b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/student-\n",
      "[nltk_data]     cum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fd4d36f-1116-4af1-9ce6-b8983bc9fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the standard stopword list\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f77c3-1265-4bfc-a76b-7c5a8ca9739b",
   "metadata": {},
   "source": [
    "### French stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88d4a3a9-4d2d-4199-8e19-290cc45cc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token not in stopwords list\n",
    "# Initialisez la liste des mots vide en français stopwords.words('french')\n",
    "\n",
    "fr_mots_vides = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5dfb8-64f4-4389-9808-125e743ddc34",
   "metadata": {},
   "source": [
    "#### Liste des mots vides en français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b307e04c-a048-429f-825f-31e339fae09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    }
   ],
   "source": [
    "print(fr_mots_vides)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf448922-9a9a-4e5d-9970-dfe8b2fa0c65",
   "metadata": {},
   "source": [
    "### English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af3c52d-e455-4ba2-923d-5716bc3f5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1f73f-70dd-4d94-8e47-d500acb76251",
   "metadata": {},
   "source": [
    "#### Liste des mots vides en Anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3562d696-990b-4b80-8070-77b18cdf1f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(en_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253409c3-707b-4071-b45d-5e996d77546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens en minuscule :\n",
      "['one', 'of', 'the', 'most', 'significant', 'contributions', 'of', 'technology', 'is', 'its', 'ability', 'to', 'enhance', 'productivity', 'automation', 'and', 'digital', 'tools', 'enable', 'businesses', 'to', 'streamline', 'processes', 'reduce', 'costs', 'and', 'increase', 'output']\n",
      " nombre : 28\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens en minuscule : mots\n",
    "print(f\"La liste des Tokens en minuscule :\\n{tokens_min}\\n nombre : {len(tokens_min)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6ab5473-1160-449a-9bb3-7ac37f2cb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   of\n",
      "2   the\n",
      "3   most\n",
      "4   of\n",
      "5   is\n",
      "6   its\n",
      "7   to\n",
      "8   and\n",
      "9   to\n",
      "10   and\n"
     ]
    }
   ],
   "source": [
    "# Boucle for\n",
    "token_stop = []\n",
    "i = 1\n",
    "for token in tokens_min :\n",
    "    if token not in en_stop_words :\n",
    "        token_stop.append(token)\n",
    "    else :\n",
    "        print(i,\" \",token)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66a84286-37be-4836-97c1-a8439d0d741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List Comprehension\n",
    "tokens_stop = [jeton for jeton in tokens_min if jeton not in en_stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc872742-16f9-4e90-9b73-4085331aac24",
   "metadata": {},
   "source": [
    "### Afficher la liste des Tokens non vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85bd7801-f22b-4207-b9be-5613b4076543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens sans stopwords :\n",
      "['one', 'significant', 'contributions', 'technology', 'ability', 'enhance', 'productivity', 'automation', 'digital', 'tools', 'enable', 'businesses', 'streamline', 'processes', 'reduce', 'costs', 'increase', 'output']\n",
      "\n",
      "Le nombre de Tokens sans stopwords :\n",
      "18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens sans mots vides\n",
    "print(f\"La liste des Tokens sans stopwords :\\n{tokens_stop}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens sans mots vides\n",
    "print(f\"Le nombre de Tokens sans stopwords :\\n{len(tokens_stop)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e39ff9-b23a-46cd-a1fa-445c1c522f4a",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43c14d4-cdec-466a-874c-b4c2f43bdae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilisez la bibliothèque SnowballStemmer pour la radicalisation.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3783494-6a65-407b-bfa9-24092678ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger le Stemmer Français\n",
    "stemmer = SnowballStemmer(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "055b7351-7c53-4106-a892-7ce339bdf090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# charger le Stemmer Anglais\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dc3436f-f1ea-4c70-ae2c-3766893834b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les stem de la liste des tokens sans stopwords tokens_stop\n",
    "tokens_stem = [stemmer.stem(token) for token in tokens_stop ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a949b34-ed5f-41e2-a698-f7b6da1d0b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Stem des Tokens :\n",
      "['one', 'signific', 'contribut', 'technolog', 'abil', 'enhanc', 'product', 'autom', 'digit', 'tool', 'enabl', 'busi', 'streamlin', 'process', 'reduc', 'cost', 'increas', 'output']\n",
      "\n",
      "Le nombre de Stem :\n",
      "18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens sans mots vides\n",
    "print(f\"La liste des Stem des Tokens :\\n{tokens_stem}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens sans mots vides\n",
    "print(f\"Le nombre de Stem :\\n{len(tokens_stem)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d859c7-03fb-4d9a-b3c5-f585278840a2",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c6c6de4-00fb-4207-9297-fe8fa0f30fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b76da32-c5f0-4a85-b5d9-625f3ddea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lem = [lemmatizer.lemmatize(token) for token in tokens_stop ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c87016b-ea92-4ac0-8136-1b3c7d05ebdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La liste des Tokens Lematiser :\n",
      "['one', 'significant', 'contribution', 'technology', 'ability', 'enhance', 'productivity', 'automation', 'digital', 'tool', 'enable', 'business', 'streamline', 'process', 'reduce', 'cost', 'increase', 'output']\n",
      "\n",
      "Le nombres de tokens Lematiser = 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher les Tokens : mots\n",
    "print(f\"La liste des Tokens Lematiser :\\n{tokens_lem}\\n\")\n",
    "\n",
    "#Afficher le nombre de Tokens\n",
    "print(f\"Le nombres de tokens Lematiser = {len(tokens_lem)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca90ab8-6b02-4555-ab37-ab6e3a96a999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
